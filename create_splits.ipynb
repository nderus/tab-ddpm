{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#create .npy data splits starting from .xlsx file (after some preprocessing)\n",
    "df = pd.read_excel(\"/home/PERSONALE/nicolas.derus2/downloads/cluster_vaccinazione_v1.2.xlsx\", engine='openpyxl', sheet_name=\"cluster_vaccinazione\")\n",
    "df['Sesso'] = pd.factorize(df['Sesso'])[0]\n",
    "df['cluster_farmaci'] = pd.factorize(df['cluster_farmaci'])[0]\n",
    "df['cluster_specialistica'] = pd.factorize(df['cluster_specialistica'])[0]\n",
    "df['Pronto Soccorso'] = df['Pronto Soccorso'].astype(int)\n",
    "df['Specialistiche'] = df['Specialistiche'].astype(int)\n",
    "df['ADI'] = df['ADI'].astype(int)\n",
    "df['Farmaci'] = df['Farmaci'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Any, Optional, cast\n",
    "import enum\n",
    "import random\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 0\n",
    "DATA_DIR = '/home/PERSONALE/nicolas.derus2/downloads/'\n",
    "EXPECTED_FILES = {\n",
    "    'cluster_vaccinazione': ['cluster_vaccinazione_v1.2.xlsx']\n",
    "}\n",
    "class TaskType(enum.Enum):\n",
    "    REGRESSION = 'regression'\n",
    "    BINCLASS = 'binclass'\n",
    "    MULTICLASS = 'multiclass'\n",
    "    \n",
    "def _start(dirname: str):\n",
    "    print(f'>>> {dirname}')\n",
    "    _set_random_seeds()\n",
    "    dataset_dir = DATA_DIR #/ dirname\n",
    "    expected_files = EXPECTED_FILES[dirname]\n",
    "    #if expected_files:\n",
    "    #    assert dataset_dir.exists()\n",
    "    #    assert set(expected_files) == set(x.name for x in dataset_dir.iterdir())\n",
    "    #else:\n",
    "    #    assert not dataset_dir.exists()\n",
    "    #    dataset_dir.mkdir()\n",
    "    return dataset_dir, expected_files\n",
    "\n",
    "\n",
    "def _set_random_seeds():\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ArrayDict = dict[str, np.ndarray]\n",
    "ArrayDict = dict()\n",
    "\n",
    "\n",
    "def _make_split(size: int, stratify: Optional[np.ndarray], n_parts: int) -> ArrayDict:\n",
    "    # n_parts == 3:      all -> train & val & test\n",
    "    # n_parts == 2: trainval -> train & val\n",
    "    assert n_parts in (2, 3)\n",
    "    all_idx = np.arange(size, dtype=np.int64)\n",
    "    a_idx, b_idx = train_test_split(\n",
    "        all_idx,\n",
    "        test_size=0.2,\n",
    "        stratify=stratify,\n",
    "        random_state=SEED + (1 if n_parts == 2 else 0),\n",
    "    )\n",
    "    if n_parts == 2:\n",
    "        return cast(ArrayDict, {'train': a_idx, 'val': b_idx})\n",
    "    a_stratify = None if stratify is None else stratify[a_idx]\n",
    "    a1_idx, a2_idx = train_test_split(\n",
    "        a_idx, test_size=0.2, stratify=a_stratify, random_state=SEED + 1\n",
    "    )\n",
    "    return cast(ArrayDict, {'train': a1_idx, 'val': a2_idx, 'test': b_idx})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _save(\n",
    "    dataset_dir,\n",
    "    name,\n",
    "    task_type,\n",
    "    *,\n",
    "    X_num,\n",
    "    X_cat,\n",
    "    y,\n",
    "    idx,\n",
    "    id_:Optional[str] = None,\n",
    "    id_suffix,\n",
    "):\n",
    "        if id_ is not None:\n",
    "            assert id_suffix == '--default'\n",
    "            \n",
    "        assert (\n",
    "            X_num is not None or X_cat is not None\n",
    "        ), 'At least one type of features must be presented.'\n",
    "        \n",
    "        if X_num is not None:\n",
    "            X_num = {k: v.astype(np.float32) for k, v in X_num.items()}\n",
    "        if X_cat is not None:\n",
    "            X_cat = {k: v.astype(str) for k, v in X_cat.items()}\n",
    "        if idx is not None:\n",
    "            idx = {k: v.astype(np.int64) for k, v in idx.items()}\n",
    "\n",
    "        y = {\n",
    "            k: v.astype(np.float32 if task_type == TaskType.REGRESSION else np.int64)\n",
    "            for k, v in y.items()\n",
    "        }\n",
    "\n",
    "        if task_type != TaskType.REGRESSION:\n",
    "            y_unique = {k: set(v.tolist()) for k, v in y.items()}\n",
    "            assert y_unique['train'] == set(range(max(y_unique['train']) + 1))\n",
    "            for x in ['val', 'test']:\n",
    "                assert y_unique[x] <= y_unique['train']\n",
    "            del x\n",
    "\n",
    "        info = {\n",
    "            'name': name,\n",
    "            'id': (dataset_dir + id_suffix) if id_ is None else id_,\n",
    "            'task_type': task_type.value,\n",
    "            'n_num_features': (0 if X_num is None else next(iter(X_num.values())).shape[1]),\n",
    "            'n_cat_features': (0 if X_cat is None else next(iter(X_cat.values())).shape[1]),\n",
    "        } | {f'{k}_size': len(v) for k, v in y.items()}\n",
    "\n",
    "\n",
    "        if task_type == TaskType.MULTICLASS:\n",
    "            info['n_classes'] = len(set(y['train']))\n",
    "        #(dataset_dir + 'info.json').write_text(json.dumps(info, indent=4)) removed .json file \n",
    "        with open(dataset_dir + 'info.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(info, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        for data_name in ['X_num', 'X_cat', 'y', 'idx']:\n",
    "            data = locals()[data_name]\n",
    "            if data is not None:\n",
    "                for k, v in data.items():\n",
    "                    np.save(dataset_dir + f'{data_name}_{k}.npy', v)\n",
    "        #(dataset_dir + 'READY').touch()\n",
    "\n",
    "        print('Done\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _apply_split(data: ArrayDict, split: ArrayDict):\n",
    "    return {k: {part: v[idx] for part, idx in split.items()} for k, v in data.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define our dataset\n",
    "\n",
    "def cluster_vaccinazione():\n",
    "    dataset_dir, files = _start('cluster_vaccinazione')\n",
    "    #df = pd.read_csv(files[0])\n",
    "    df = pd.read_excel(dataset_dir + files[0])\n",
    "    df = df.drop(columns=['Id', 'FasciaEta']) \n",
    "    df['Sesso'] = pd.factorize(df['Sesso'])[0]\n",
    "    df['cluster_farmaci'] = pd.factorize(df['cluster_farmaci'])[0]\n",
    "    df['cluster_specialistica'] = pd.factorize(df['cluster_specialistica'])[0]\n",
    "    df['Pronto Soccorso'] = df['Pronto Soccorso'].astype(int)\n",
    "    df['Specialistiche'] = df['Specialistiche'].astype(int)\n",
    "    df['ADI'] = df['ADI'].astype(int)\n",
    "    df['Farmaci'] = df['Farmaci'].astype(int)\n",
    "    #df['Sesso'] = df['Sesso'].astype('category').cat.codes.values.astype(np.int64)\n",
    "    y_all = df.pop('cluster_doc').values.astype(np.int64)\n",
    "    num_columns = [\n",
    "        'Eta',\n",
    "        'Comorbilita',\n",
    "        'Ricoveri',\n",
    "        'Pronto Soccorso',\n",
    "        'Specialistiche',\n",
    "        'ADI',\n",
    "        'Farmaci',\n",
    "        \n",
    "    ]\n",
    "    cat_columns = ['Sesso','cluster_specialistica', 'cluster_farmaci',\n",
    "       'malattie_cardiovascolari_croniche', 'malattie_cardiovascolari_acute',\n",
    "       'malattie_respiratorie', 'malattie_respiratorie_acute',\n",
    "       'malattie_metaboliche', 'deficit_immunitari', 'tumori', 'trapianti',\n",
    "       'condizioni_neurologiche_disabilita', 'fibrosi_cistica',\n",
    "       'malattia_renale', 'malattie_cerebrovascolari', 'malattie_epatiche',\n",
    "       'obesita_grave', 'sindrome_down',\n",
    "        ]\n",
    "    assert set(num_columns) | set(cat_columns) == set(df.columns.tolist())\n",
    "    X_num_all = df[num_columns].astype(np.float32).values\n",
    "    X_cat_all = df[cat_columns].astype(str).values\n",
    "    idx = _make_split(len(df), y_all, 3)\n",
    "\n",
    "    _save(dataset_dir,\n",
    "        'cluster_vaccinazione',\n",
    "        TaskType.MULTICLASS,\n",
    "        **_apply_split(\n",
    "            {'X_num': X_num_all, 'X_cat': X_cat_all, 'y': y_all}, idx,\n",
    "        ),\n",
    "        idx=idx,\n",
    "        id_suffix = '--default',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_vaccinazione()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
